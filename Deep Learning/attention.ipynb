{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1138027d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(5)\n",
    "np.random.seed(5)\n",
    "torch.random.manual_seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([-2.0, 1.0, 0.5])\n",
    "x2 = np.array([1.0, 1.5, -0.5])\n",
    "x3 = np.array([-1.5, 1.0, -0.5])\n",
    "x4 = np.array([-2.0, -2.5, 1.5])\n",
    "\n",
    "X = np.array([x1, x2, x3, x4])\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[4.5 0.  4.5 0. ]\n",
      "[2.59807621 0.         2.59807621 0.        ]\n",
      "[0.46536883 0.03463117 0.46536883 0.03463117]\n",
      "[-1.66342208  0.8961065   0.03463117]\n"
     ]
    }
   ],
   "source": [
    "q = np.array([-2.0, 1.0, -1.0])\n",
    "\n",
    "print(np.size(q))\n",
    "\n",
    "scores = X.dot(q) / np.sqrt(np.size(q))\n",
    "probabilities = np.exp(scores) / np.sum(np.exp(scores))\n",
    "output = X.T.dot(probabilities)\n",
    "\n",
    "print(X.dot(q))\n",
    "print(scores)\n",
    "print(probabilities)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.25  4.5 ]\n",
      " [ 1.25  2.  ]\n",
      " [-1.25  4.75]\n",
      " [-2.75 -3.5 ]]\n",
      "[[ 5.75  1.5 ]\n",
      " [ 2.   -0.5 ]\n",
      " [ 4.5   2.  ]\n",
      " [-2.5   0.5 ]]\n",
      "[[-2.5  -7.5 ]\n",
      " [ 0.25  0.  ]\n",
      " [-2.   -5.25]\n",
      " [-0.75 -1.5 ]]\n",
      "1.4142135623730951\n",
      "[[ -4.37522321  -4.77297077  -0.79549513   5.5684659 ]\n",
      " [  7.20365033   1.06066017   6.80590277  -1.50260191]\n",
      " [ -0.04419417  -3.44714556   2.74003878   3.8890873 ]\n",
      " [-14.89343658  -2.65165043 -13.70019389   3.62392225]]\n",
      "[[4.79433566e-05 3.22098620e-05 1.71943035e-03 9.98200416e-01]\n",
      " [5.97319598e-01 1.28333499e-03 4.01298182e-01 9.88847812e-05]\n",
      " [1.46423661e-02 4.87223528e-04 2.37021787e-01 7.47848624e-01]\n",
      " [9.06143069e-09 1.87817885e-03 2.98824011e-08 9.98121782e-01]]\n",
      "[[-0.75220098 -1.50668721]\n",
      " [-2.29564869 -6.58686077]\n",
      " [-1.07141415 -2.47595506]\n",
      " [-0.74812187 -1.4971829 ]]\n",
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "W_Q = np.array([[1, -1.5], [0, 2], [-0.5, -1]])\n",
    "W_K = np.array([[-1.5, -1], [2.5, 0], [0.5, -1]])\n",
    "W_V = np.array([[1, 2.5], [-0.5, -2], [0, -1]])\n",
    "\n",
    "#print(W_Q)\n",
    "#print(W_K)\n",
    "#print(W_V)\n",
    "\n",
    "Q = X.dot(W_Q)\n",
    "K = X.dot(W_K)\n",
    "V = X.dot(W_V)\n",
    "\n",
    "print(Q)\n",
    "print(K)\n",
    "print(V)\n",
    "\n",
    "print(np.sqrt(np.size(Q, 1)))\n",
    "\n",
    "scores = Q.dot(K.T) / np.sqrt(np.size(Q, 1))\n",
    "probabilities = np.exp(scores) / np.sum(np.exp(scores), axis=1)[:, None]\n",
    "\n",
    "Z = probabilities.dot(V)\n",
    "\n",
    "print(scores)\n",
    "print(probabilities)\n",
    "print(Z)\n",
    "\n",
    "print(probabilities.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15527ea60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.79433566e-05 3.22098620e-05 1.71943035e-03 9.98200416e-01]\n",
      " [5.97319598e-01 1.28333499e-03 4.01298182e-01 9.88847812e-05]\n",
      " [1.46423661e-02 4.87223528e-04 2.37021787e-01 7.47848624e-01]\n",
      " [9.06143069e-09 1.87817885e-03 2.98824011e-08 9.98121782e-01]]\n",
      "[[-0.75220098 -1.50668721]\n",
      " [-2.29564869 -6.58686077]\n",
      " [-1.07141415 -2.47595506]\n",
      " [-0.74812187 -1.4971829 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.18306921e-01 2.01966211e-02 1.68483136e-01 6.93013322e-01]\n",
      " [8.48429312e-04 9.98944583e-01 2.06267364e-04 7.20592823e-07]\n",
      " [2.67590116e-02 7.79843042e-04 5.42703523e-02 9.18190793e-01]\n",
      " [2.47463407e-05 6.12522986e-10 2.06437556e-04 9.99768815e-01]]\n",
      "[[-2.26628332 -2.26628332]\n",
      " [ 1.99725652  1.99725652]\n",
      " [-2.82066255 -2.82066255]\n",
      " [-2.99952526 -2.99952526]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75220098 -1.50668721 -2.26628332 -2.26628332]\n",
      " [-2.29564869 -6.58686077  1.99725652  1.99725652]\n",
      " [-1.07141415 -2.47595506 -2.82066255 -2.82066255]\n",
      " [-0.74812187 -1.4971829  -2.99952526 -2.99952526]]\n",
      "[[-6.04664898  3.77781072 -0.75731086]\n",
      " [ 8.28741825  0.14750295 10.57968068]\n",
      " [-7.3905735   5.09982766 -0.01158073]\n",
      " [-8.25045389  4.87428797 -1.50140321]]\n"
     ]
    }
   ],
   "source": [
    "W_O = np.array([[-1, 1.5, 2], [0, -1, -2], [1, -1.5, 0], [2, 0, 1]])\n",
    "\n",
    "W_Q_heads = [W_Q, np.ones_like(W_Q)]\n",
    "W_K_heads = [W_K, np.ones_like(W_K)]\n",
    "W_V_heads = [W_V, np.ones_like(W_V)]\n",
    "\n",
    "head_representations = []\n",
    "\n",
    "for W_Q_h, W_K_h, W_V_h in zip(W_Q_heads, W_K_heads, W_V_heads):\n",
    "    Q_h = X.dot(W_Q_h)\n",
    "    K_h = X.dot(W_K_h)\n",
    "    V_h = X.dot(W_V_h)\n",
    "    scores = Q_h.dot(K_h.T) / np.sqrt(np.size(Q_h, 1))\n",
    "    probabilities = np.exp(scores) / np.sum(np.exp(scores), axis=1)[:, None]\n",
    "    Z_h = probabilities.dot(V_h)\n",
    "    head_representations.append(Z_h)\n",
    "    print(probabilities)\n",
    "    print(Z_h)\n",
    "    plt.imshow(probabilities)\n",
    "    plt.show()\n",
    "    \n",
    "print(np.concatenate(head_representations, axis=1))\n",
    "\n",
    "Z = np.concatenate(head_representations, axis=1).dot(W_O)\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a sequence-to-sequence network that reverses strings with the help of attention. We will randomly generate strings consisting of \"a\", \"b\", \"c\", and \"d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accdab', 'bbcdadccbdab', 'bbbdccb', 'ccdca', 'dcc', 'acaaa', 'dcdb', 'bdbdb', 'bbdcbccdd', 'baad']\n"
     ]
    }
   ],
   "source": [
    "BOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "\n",
    "raw_vocab = list(\"abcd\")\n",
    "itos = [BOS, EOS] + raw_vocab\n",
    "stoi = {n: i for i, n in enumerate(itos)}\n",
    "vocab_size = len(itos)  # Plus BOS/EOS\n",
    "\n",
    "N = 200\n",
    "valid_size = 100\n",
    "\n",
    "def sample_string(min_length, max_length):\n",
    "    length = random.randrange(min_length, max_length)\n",
    "    return \"\".join([random.choice(raw_vocab) for _ in range(length)])\n",
    "\n",
    "def sample_strings(min_length, max_length, size):\n",
    "    return [sample_string(min_length, max_length) for _ in range(size)]\n",
    "\n",
    "def to_tensor(name):\n",
    "    indices = [stoi[BOS]] + [stoi[n] for n in name] + [stoi[EOS]]\n",
    "    return torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "def make_dataset(lines):\n",
    "    dataset = [(to_tensor(line), to_tensor(reversed(line))) for line in lines]\n",
    "    return dataset\n",
    "\n",
    "train_lines = sample_strings(3, 15, N)\n",
    "valid_lines = sample_strings(3, 15, valid_size)\n",
    "\n",
    "train_dataset = make_dataset(train_lines)\n",
    "valid_dataset = make_dataset(valid_lines)\n",
    "\n",
    "print(train_lines[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the model is an RNN-based encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, bidirectional=False):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        if bidirectional:\n",
    "            hidden_size //= 2\n",
    "        self.rnn = nn.LSTM(\n",
    "            embedding_size, \n",
    "            hidden_size, \n",
    "            bidirectional=bidirectional, \n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \"\"\"\n",
    "        input (LongTensor): batch x src length\n",
    "        src length (batch-length list0: If given, the input will be packed\n",
    "        hidden: hidden or hidden/cell state input dimensions for the RNN type\n",
    "        returns:\n",
    "            output (FloatTensor): batch x src length x hidden size\n",
    "            hidden_n (FloatTensor): hidden or hidden/cell state input\n",
    "                dimensions for the RNN type\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(input)\n",
    "        output, hidden_n = self.rnn(emb, hidden)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden_n = self._reshape_hidden(hidden_n)\n",
    "        return output, hidden_n\n",
    "\n",
    "    def _merge_tensor(self, state_tensor):\n",
    "        forward_states = state_tensor[::2]\n",
    "        backward_states = state_tensor[1::2]\n",
    "        return torch.cat([forward_states, backward_states], 2)\n",
    "\n",
    "    def _reshape_hidden(self, hidden):\n",
    "        \"\"\"\n",
    "        hidden:\n",
    "            num_layers * num_directions x batch x self.hidden_size // 2\n",
    "            or a tuple of these\n",
    "        returns:\n",
    "            num_layers\n",
    "        \"\"\"\n",
    "        assert self.rnn.bidirectional\n",
    "        if isinstance(hidden, tuple):\n",
    "            return tuple(self._merge_tensor(h) for h in hidden)\n",
    "        else:\n",
    "            return self._merge_tensor(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define a decoder. This implementation works both with and without an attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, attn=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
    "        self.attn = attn\n",
    "\n",
    "    def forward(self, input, context, hidden):\n",
    "        \"\"\"\n",
    "        input (LongTensor): batch x tgt length\n",
    "        context (FloatTensor): batch x src length x hidden size\n",
    "        hidden: hidden or hidden/cell state input dimensions for the RNN type\n",
    "        returns (FloatTensor): (batch*tgt length) x output size\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(input)\n",
    "        output, hidden_n = self.rnn(emb, hidden)\n",
    "\n",
    "        alignment = None\n",
    "        # apply attention between source context and query from\n",
    "        # decoder RNN\n",
    "        if self.attn is not None:\n",
    "            output, alignment = self.attn(output, context)\n",
    "\n",
    "        flat_output = output.contiguous().view(-1, self.rnn.hidden_size)\n",
    "        return self.output_layer(flat_output), alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put them together into an encoder-decoder model class, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src, tgt (LongTensor): (batch size x sequence length)\n",
    "        returns (FloatTensor): (batch*tgt length) x output size\n",
    "        \"\"\"\n",
    "        context, enc_hidden = self.encoder(src)\n",
    "        return self.decoder(tgt, context=context, hidden=enc_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our base model defined, we can write training and validation code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_iter, loss, optimizer):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    random.shuffle(train_iter)  # present examples in random order\n",
    "    for src, tgt in train_iter:\n",
    "        model.zero_grad()\n",
    "        tgt_in = tgt[:, :-1]\n",
    "        pred, _ = model(src, tgt_in)\n",
    "        gold = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        batch_loss = loss(pred, gold)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += batch_loss.item()\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validate(model, data_iter):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_iter:\n",
    "            tgt_in = tgt[:, :-1]\n",
    "            pred = model(src, tgt_in)[0].argmax(dim=1)\n",
    "            gold = tgt[:, 1:].contiguous().view(-1)\n",
    "            n_correct += (pred == gold).sum().item()\n",
    "            n_total += gold.size(0)\n",
    "        return n_correct / n_total\n",
    "\n",
    "\n",
    "def train(model, train, valid, epochs=30, learning_rate=0.5):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_accs = []\n",
    "    epochs = list(range(1, epochs + 1))\n",
    "    for epoch in epochs:\n",
    "        print('Training epoch {}'.format(epoch))\n",
    "        train_loss = train_epoch(model, train, loss, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_acc = validate(model, valid)\n",
    "        valid_accs.append(valid_acc)\n",
    "        print('Train loss: {} ; Validation acc: {}'.format(train_loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a unidirectional model without attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embeddings): Embedding(6, 6)\n",
      "    (rnn): LSTM(6, 64, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embeddings): Embedding(6, 6)\n",
      "    (rnn): LSTM(6, 64, batch_first=True)\n",
      "    (output_layer): Linear(in_features=64, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "Training epoch 1\n",
      "Train loss: 322.14679288864136 ; Validation acc: 0.22698744769874477\n",
      "Training epoch 2\n",
      "Train loss: 313.6431759595871 ; Validation acc: 0.2405857740585774\n",
      "Training epoch 3\n",
      "Train loss: 311.3582640886307 ; Validation acc: 0.2489539748953975\n",
      "Training epoch 4\n",
      "Train loss: 301.8337528705597 ; Validation acc: 0.3274058577405858\n",
      "Training epoch 5\n",
      "Train loss: 278.1469255685806 ; Validation acc: 0.38284518828451886\n",
      "Training epoch 6\n",
      "Train loss: 249.31653499603271 ; Validation acc: 0.3378661087866109\n",
      "Training epoch 7\n",
      "Train loss: 233.24049365520477 ; Validation acc: 0.41736401673640167\n",
      "Training epoch 8\n",
      "Train loss: 218.09211057424545 ; Validation acc: 0.5177824267782427\n",
      "Training epoch 9\n",
      "Train loss: 198.24607175588608 ; Validation acc: 0.5439330543933054\n",
      "Training epoch 10\n",
      "Train loss: 176.39644861221313 ; Validation acc: 0.6286610878661087\n",
      "Training epoch 11\n",
      "Train loss: 168.96592788398266 ; Validation acc: 0.6234309623430963\n",
      "Training epoch 12\n",
      "Train loss: 147.41174985468388 ; Validation acc: 0.6359832635983264\n",
      "Training epoch 13\n",
      "Train loss: 133.81463658064604 ; Validation acc: 0.7071129707112971\n",
      "Training epoch 14\n",
      "Train loss: 121.58452330157161 ; Validation acc: 0.6903765690376569\n",
      "Training epoch 15\n",
      "Train loss: 112.16542560979724 ; Validation acc: 0.7322175732217573\n",
      "Training epoch 16\n",
      "Train loss: 103.40525299496949 ; Validation acc: 0.7510460251046025\n",
      "Training epoch 17\n",
      "Train loss: 93.49677292816341 ; Validation acc: 0.7615062761506276\n",
      "Training epoch 18\n",
      "Train loss: 93.51675171591341 ; Validation acc: 0.7646443514644351\n",
      "Training epoch 19\n",
      "Train loss: 84.99395050294697 ; Validation acc: 0.7646443514644351\n",
      "Training epoch 20\n",
      "Train loss: 78.40645131468773 ; Validation acc: 0.7583682008368201\n",
      "Training epoch 21\n",
      "Train loss: 77.73401167523116 ; Validation acc: 0.7405857740585774\n",
      "Training epoch 22\n",
      "Train loss: 71.60308619821444 ; Validation acc: 0.7887029288702929\n",
      "Training epoch 23\n",
      "Train loss: 67.14971800334752 ; Validation acc: 0.8043933054393305\n",
      "Training epoch 24\n",
      "Train loss: 61.67566619790159 ; Validation acc: 0.8158995815899581\n",
      "Training epoch 25\n",
      "Train loss: 62.28468349785544 ; Validation acc: 0.8347280334728033\n",
      "Training epoch 26\n",
      "Train loss: 53.41920166416094 ; Validation acc: 0.8232217573221757\n",
      "Training epoch 27\n",
      "Train loss: 53.42857728141826 ; Validation acc: 0.8305439330543933\n",
      "Training epoch 28\n",
      "Train loss: 50.38818551111035 ; Validation acc: 0.8242677824267782\n",
      "Training epoch 29\n",
      "Train loss: 45.88566068850923 ; Validation acc: 0.8326359832635983\n",
      "Training epoch 30\n",
      "Train loss: 41.418330186395906 ; Validation acc: 0.8211297071129707\n"
     ]
    }
   ],
   "source": [
    "embedding_size = vocab_size\n",
    "hidden_size = 64\n",
    "\n",
    "enc = Encoder(vocab_size, embedding_size, hidden_size)\n",
    "dec = Decoder(vocab_size, embedding_size, hidden_size)\n",
    "enc.embeddings.weight.data = torch.eye(vocab_size)\n",
    "dec.embeddings.weight.data = enc.embeddings.weight.data\n",
    "enc.embeddings.weight.requires_grad = False\n",
    "dec.embeddings.weight.requires_grad = False\n",
    "\n",
    "model = Seq2Seq(enc, dec)\n",
    "print(model)\n",
    "\n",
    "train(model, train_dataset, valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model often manages to predict the right sequence, but it also often fails. Note that the decoder makes use of the *last* hidden state from the encoder, which has recently seen the final time step of the source sequence (in other words, the first element it needs to predict), but the other elements less recently. If only there were a way to make it easier for the model to focus on less recent positions...\n",
    "\n",
    "The attention mechanism is an extra layer for the decoder that can do precisely this. In this exercise, we consider a simple but effective attention mechanism called *dot product attention*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProdAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, query, context):\n",
    "        \"\"\"\n",
    "        query: batch x tgt_length x hidden_size\n",
    "        context: batch x src_length x hidden_size\n",
    "        \"\"\"\n",
    "        tgt_batch, tgt_len, tgt_hidden = query.size()\n",
    "        src_batch, src_len, src_hidden = context.size()\n",
    "        attn_scores = torch.bmm(query, context.transpose(1, 2))\n",
    "        alignment = torch.softmax(attn_scores, 2)\n",
    "        c = torch.bmm(alignment, context)\n",
    "        attn_h_t = self.mlp(torch.cat([c, query], dim=2))\n",
    "        return attn_h_t, alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that attention has been implemented, we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embeddings): Embedding(6, 6)\n",
      "    (rnn): LSTM(6, 64, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embeddings): Embedding(6, 6)\n",
      "    (rnn): LSTM(6, 64, batch_first=True)\n",
      "    (output_layer): Linear(in_features=64, out_features=6, bias=True)\n",
      "    (attn): DotProdAttention(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Training epoch 1\n",
      "Train loss: 323.62988901138306 ; Validation acc: 0.2615062761506276\n",
      "Training epoch 2\n",
      "Train loss: 313.2430661916733 ; Validation acc: 0.23430962343096234\n",
      "Training epoch 3\n",
      "Train loss: 296.82833790779114 ; Validation acc: 0.33158995815899583\n",
      "Training epoch 4\n",
      "Train loss: 265.0910856127739 ; Validation acc: 0.21443514644351463\n",
      "Training epoch 5\n",
      "Train loss: 239.73637267947197 ; Validation acc: 0.6150627615062761\n",
      "Training epoch 6\n",
      "Train loss: 194.75810405611992 ; Validation acc: 0.6244769874476988\n",
      "Training epoch 7\n",
      "Train loss: 147.13418799638748 ; Validation acc: 0.7677824267782427\n",
      "Training epoch 8\n",
      "Train loss: 120.90554459579289 ; Validation acc: 0.8221757322175732\n",
      "Training epoch 9\n",
      "Train loss: 63.057826748117805 ; Validation acc: 0.9173640167364017\n",
      "Training epoch 10\n",
      "Train loss: 75.20601192023605 ; Validation acc: 0.893305439330544\n",
      "Training epoch 11\n",
      "Train loss: 29.1826205101097 ; Validation acc: 0.9341004184100419\n",
      "Training epoch 12\n",
      "Train loss: 11.267480417591287 ; Validation acc: 0.9518828451882845\n",
      "Training epoch 13\n",
      "Train loss: 7.664132998907007 ; Validation acc: 0.997907949790795\n",
      "Training epoch 14\n",
      "Train loss: 0.5066078952004318 ; Validation acc: 0.997907949790795\n",
      "Training epoch 15\n",
      "Train loss: 0.2902472220812342 ; Validation acc: 0.9989539748953975\n",
      "Training epoch 16\n",
      "Train loss: 0.15086157252881094 ; Validation acc: 1.0\n",
      "Training epoch 17\n",
      "Train loss: 0.10621372225432424 ; Validation acc: 1.0\n",
      "Training epoch 18\n",
      "Train loss: 0.08581572886760114 ; Validation acc: 1.0\n",
      "Training epoch 19\n",
      "Train loss: 0.07235289266282052 ; Validation acc: 0.9989539748953975\n",
      "Training epoch 20\n",
      "Train loss: 0.06255052968117525 ; Validation acc: 0.9989539748953975\n",
      "Training epoch 21\n",
      "Train loss: 0.05528530306946777 ; Validation acc: 0.9989539748953975\n",
      "Training epoch 22\n",
      "Train loss: 0.04947523711962276 ; Validation acc: 0.9989539748953975\n",
      "Training epoch 23\n",
      "Train loss: 0.04476701659223181 ; Validation acc: 0.9989539748953975\n",
      "Training epoch 24\n",
      "Train loss: 0.040901930355175864 ; Validation acc: 0.9989539748953975\n",
      "Training epoch 25\n",
      "Train loss: 0.03764354890336108 ; Validation acc: 0.9989539748953975\n",
      "Training epoch 26\n",
      "Train loss: 0.03486947840065113 ; Validation acc: 0.9989539748953975\n",
      "Training epoch 27\n",
      "Train loss: 0.032473872270202264 ; Validation acc: 0.9989539748953975\n",
      "Training epoch 28\n",
      "Train loss: 0.030398341389627603 ; Validation acc: 0.9989539748953975\n",
      "Training epoch 29\n",
      "Train loss: 0.0285614324548078 ; Validation acc: 0.9989539748953975\n",
      "Training epoch 30\n",
      "Train loss: 0.026932926466542995 ; Validation acc: 0.9989539748953975\n"
     ]
    }
   ],
   "source": [
    "attn = DotProdAttention(hidden_size)\n",
    "enc = Encoder(vocab_size, embedding_size, hidden_size, bidirectional=False)\n",
    "dec = Decoder(vocab_size, embedding_size, hidden_size, attn=attn)\n",
    "enc.embeddings.weight.data = torch.eye(vocab_size)\n",
    "dec.embeddings.weight.data = enc.embeddings.weight.data\n",
    "enc.embeddings.weight.requires_grad = False\n",
    "dec.embeddings.weight.requires_grad = False\n",
    "\n",
    "attn_model = Seq2Seq(enc, dec)\n",
    "print(attn_model)\n",
    "\n",
    "train(attn_model, train_dataset, valid_dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the model's attention matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1556c4940>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "string = \"abacadabacc\"  # try something\n",
    "reversed_string = reversed(string)\n",
    "\n",
    "src = to_tensor(string)\n",
    "tgt = to_tensor(reversed_string)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, alignment = attn_model(src, tgt)\n",
    "\n",
    "attn_matrix = alignment.squeeze(0).numpy()\n",
    "plt.matshow(attn_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
